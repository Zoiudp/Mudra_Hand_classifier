{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5a9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# After model prediction\\nfor features, labels in val_loader:\\n    batch_size = features.size(0)\\n    predicted_classes, probabilities, attention_weights = predict_with_attention(\\n        model, features, label_encoder, device\\n    )\\n    \\n    # Get sample information\\n    sample_idx = 0  # First sample in batch\\n    true_class = label_encoder.inverse_transform([labels[sample_idx].item()])[0]\\n    predicted_class = predicted_classes[sample_idx]\\n    \\n    # Visualize with enhanced functions\\n    enhanced_attn_file = enhanced_attention_matrix(\\n        attention_weights, \\n        seq_length,\\n        title=\"Mudra Attention Pattern\",\\n        layer_idx=num_encoder_layers-1,  # Last layer\\n        head_idx=0,  # First head\\n        sample_idx=sample_idx,\\n        predicted_class=predicted_class,\\n        true_class=true_class\\n    )\\n    \\n    # Create all-heads visualization\\n    all_heads_file = visualize_all_attention_heads_enhanced(\\n        attention_weights,\\n        seq_length,\\n        num_encoder_layers,\\n        nhead,\\n        sample_idx=sample_idx,\\n        predicted_class=predicted_class,\\n        true_class=true_class\\n    )\\n    \\n    # Create a comprehensive summary\\n    summary_file = create_attention_pattern_summary(\\n        attention_weights,\\n        seq_length,\\n        num_encoder_layers,\\n        nhead,\\n        sample_idx=sample_idx,\\n        predicted_class=predicted_class,\\n        true_class=true_class\\n    )\\n    \\n    print(f\"Enhanced attention visualization saved to: {enhanced_attn_file}\")\\n    print(f\"All heads visualization saved to: {all_heads_file}\")\\n    print(f\"Attention pattern summary saved to: {summary_file}\")\\n    \\n    break  # Just process one batch for example\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def enhanced_attention_matrix(attention_weights, seq_length, title=\"Enhanced Attention Matrix\", \n",
    "                              layer_idx=0, head_idx=0, sample_idx=0,\n",
    "                              feature_names=None, timestamp_data=None, \n",
    "                              predicted_class=None, true_class=None,\n",
    "                              cmap='viridis'):\n",
    "    \"\"\"\n",
    "    Creates an enhanced visualization of the attention matrix with better labeling and context.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: List of attention weight tensors from the transformer model\n",
    "        seq_length: Length of the input sequence\n",
    "        title: Base title for the plot\n",
    "        layer_idx: Index of the transformer layer to visualize\n",
    "        head_idx: Index of the attention head to visualize\n",
    "        sample_idx: Index of the sample in the batch to visualize\n",
    "        feature_names: List of names for the input features (for context)\n",
    "        timestamp_data: Time data for the sequence (if available)\n",
    "        predicted_class: Model's prediction for this sample\n",
    "        true_class: Ground truth label for this sample\n",
    "        cmap: Colormap for the heatmap\n",
    "    \n",
    "    Returns:\n",
    "        Path to the saved visualization\n",
    "    \"\"\"\n",
    "    # Create a figure with two subplots - main heatmap and a zoomed-in view\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Define a custom colormap that better highlights the attention patterns\n",
    "    custom_cmap = sns.color_palette(\"YlGnBu\", as_cmap=True)\n",
    "    \n",
    "    # Create a grid for the subplots\n",
    "    gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[3, 1], width_ratios=[3, 1])\n",
    "    \n",
    "    # Main heatmap - upper left\n",
    "    ax_main = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    # Extract attention matrix - handle different return formats\n",
    "    if isinstance(attention_weights[layer_idx], torch.Tensor):\n",
    "        attn_shape = attention_weights[layer_idx].shape\n",
    "        \n",
    "        if len(attn_shape) == 4:  # [batch_size, nhead, seq_len, seq_len]\n",
    "            attn_matrix = attention_weights[layer_idx][sample_idx, head_idx].cpu().numpy()\n",
    "        elif len(attn_shape) == 3:  # [batch_size, seq_len, seq_len]\n",
    "            attn_matrix = attention_weights[layer_idx][sample_idx].cpu().numpy()\n",
    "        else:\n",
    "            attn_matrix = np.ones((seq_length, seq_length)) / seq_length\n",
    "    else:\n",
    "        try:\n",
    "            attn_matrix = attention_weights[layer_idx][0][sample_idx, head_idx].cpu().numpy()\n",
    "        except (TypeError, IndexError):\n",
    "            attn_matrix = np.ones((seq_length, seq_length)) / seq_length\n",
    "    \n",
    "    # Create tick labels based on position or timestamp if available\n",
    "    if timestamp_data is not None:\n",
    "        x_labels = [f\"t{i}: {timestamp_data[i]:.2f}s\" for i in range(seq_length)]\n",
    "    else:\n",
    "        x_labels = [f\"t{i}\" for i in range(seq_length)]\n",
    "    \n",
    "    # Plot the main heatmap\n",
    "    sns.heatmap(attn_matrix, annot=False, cmap=custom_cmap, ax=ax_main, \n",
    "                cbar=True, cbar_kws={'label': 'Attention Weight'})\n",
    "    \n",
    "    # Add labels and title with model prediction info\n",
    "    prediction_info = \"\"\n",
    "    if predicted_class and true_class:\n",
    "        match_status = \"✓\" if predicted_class == true_class else \"✗\"\n",
    "        prediction_info = f\"\\nPredicted: {predicted_class} | Actual: {true_class} {match_status}\"\n",
    "    \n",
    "    ax_main.set_title(f\"{title} - Layer {layer_idx+1}, Head {head_idx+1}{prediction_info}\", \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    ax_main.set_xlabel(\"Target Position in Sequence\", fontsize=12)\n",
    "    ax_main.set_ylabel(\"Source Position in Sequence\", fontsize=12)\n",
    "    \n",
    "    # Improve tick labels - use nice formatting and rotation\n",
    "    ax_main.set_xticks(np.arange(len(x_labels))+0.5)\n",
    "    ax_main.set_yticks(np.arange(len(x_labels))+0.5)\n",
    "    ax_main.set_xticklabels(x_labels, rotation=45, ha='right', fontsize=10)\n",
    "    ax_main.set_yticklabels(x_labels, rotation=0, fontsize=10)\n",
    "    \n",
    "    # Add a grid to make it easier to follow\n",
    "    ax_main.grid(False)\n",
    "    \n",
    "    # Highlight the diagonal if appropriate\n",
    "    for i in range(seq_length):\n",
    "        ax_main.add_patch(plt.Rectangle((i, i), 1, 1, fill=False, edgecolor='black', lw=1))\n",
    "    \n",
    "    # Plot row-wise attention sums - right panel\n",
    "    ax_row_sum = fig.add_subplot(gs[0, 1])\n",
    "    row_sums = attn_matrix.sum(axis=1)\n",
    "    \n",
    "    # Make a horizontal barplot\n",
    "    row_bars = ax_row_sum.barh(np.arange(len(row_sums)), row_sums, \n",
    "                               color=plt.cm.get_cmap(cmap)(row_sums/row_sums.max()))\n",
    "    ax_row_sum.set_yticks(np.arange(len(x_labels)))\n",
    "    ax_row_sum.set_yticklabels([])  # No need to duplicate the labels\n",
    "    ax_row_sum.set_xlim(0, max(row_sums)*1.1)\n",
    "    ax_row_sum.set_title(\"Row Attention Sum\", fontsize=12)\n",
    "    ax_row_sum.set_xlabel(\"Sum of Weights\", fontsize=10)\n",
    "    \n",
    "    # Plot column-wise attention sums - bottom panel\n",
    "    ax_col_sum = fig.add_subplot(gs[1, 0])\n",
    "    col_sums = attn_matrix.sum(axis=0)\n",
    "    \n",
    "    # Make a vertical barplot\n",
    "    col_bars = ax_col_sum.bar(np.arange(len(col_sums)), col_sums,\n",
    "                              color=plt.cm.get_cmap(cmap)(col_sums/col_sums.max()))\n",
    "    ax_col_sum.set_xticks(np.arange(len(x_labels)))\n",
    "    ax_col_sum.set_xticklabels([])  # No need to duplicate the labels\n",
    "    ax_col_sum.set_ylim(0, max(col_sums)*1.1)\n",
    "    ax_col_sum.set_title(\"Column Attention Sum\", fontsize=12)\n",
    "    ax_col_sum.set_ylabel(\"Sum of Weights\", fontsize=10)\n",
    "    \n",
    "    # Add model info and matrix statistics in bottom right\n",
    "    ax_info = fig.add_subplot(gs[1, 1])\n",
    "    ax_info.axis('off')  # No axis for text box\n",
    "    \n",
    "    # Calculate some stats about the attention matrix\n",
    "    stats_text = (\n",
    "        f\"Matrix Statistics:\\n\"\n",
    "        f\"  Min: {attn_matrix.min():.3f}\\n\"\n",
    "        f\"  Max: {attn_matrix.max():.3f}\\n\"\n",
    "        f\"  Mean: {attn_matrix.mean():.3f}\\n\"\n",
    "        f\"  Std: {attn_matrix.std():.3f}\\n\\n\"\n",
    "        f\"Key Position Focus:\\n\"\n",
    "        f\"  Strongest source: t{np.argmax(row_sums)}\\n\"\n",
    "        f\"  Strongest target: t{np.argmax(col_sums)}\"\n",
    "    )\n",
    "    \n",
    "    # Add the stats text\n",
    "    ax_info.text(0, 0.95, stats_text, fontsize=10, \n",
    "                verticalalignment='top', family='monospace')\n",
    "    \n",
    "    # Add a caption explaining what attention means\n",
    "    caption = (\n",
    "        \"The attention matrix shows how each position in the sequence\\n\"\n",
    "        \"attends to other positions. Brighter colors indicate stronger\\n\"\n",
    "        \"attention weights. The row sums show which source positions\\n\"\n",
    "        \"are most influential, while column sums show which target\\n\"\n",
    "        \"positions receive the most attention.\"\n",
    "    )\n",
    "    ax_info.text(0, 0.3, caption, fontsize=9, style='italic',\n",
    "                verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.15, hspace=0.2)\n",
    "    \n",
    "    # Save the figure\n",
    "    save_path = f'enhanced_attn_layer{layer_idx+1}_head{head_idx+1}.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "def visualize_all_attention_heads_enhanced(attention_weights, seq_length, num_layers, num_heads, \n",
    "                                          sample_idx=0, predicted_class=None, true_class=None):\n",
    "    \"\"\"\n",
    "    Creates an enhanced visualization of attention matrices for all heads and layers.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: List of attention weight tensors\n",
    "        seq_length: Length of input sequence\n",
    "        num_layers: Number of transformer layers\n",
    "        num_heads: Number of attention heads per layer\n",
    "        sample_idx: Index of the sample in the batch\n",
    "        predicted_class: Model's prediction for this sample\n",
    "        true_class: Ground truth label for this sample\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved visualization\n",
    "    \"\"\"\n",
    "    # Use a larger figure size to accommodate all heads and layers\n",
    "    fig_width = max(12, num_heads * 2.5)\n",
    "    fig_height = max(8, num_layers * 2.5)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_layers, num_heads, \n",
    "                            figsize=(fig_width, fig_height),\n",
    "                            squeeze=False)\n",
    "    \n",
    "    # Use a consistent colormap\n",
    "    cmap = \"YlGnBu\"  # This colormap works well for attention visualization\n",
    "    \n",
    "    # Add a super title with prediction information\n",
    "    if predicted_class and true_class:\n",
    "        match_status = \"Correct ✓\" if predicted_class == true_class else \"Incorrect ✗\"\n",
    "        plt.suptitle(f\"Attention Patterns Across All Layers and Heads\\n\" + \n",
    "                    f\"Prediction: {predicted_class} | Actual: {true_class} ({match_status})\",\n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "    else:\n",
    "        plt.suptitle(\"Attention Patterns Across All Layers and Heads\", \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "    # Create attention matrix for each layer and head\n",
    "    max_value = 0  # Track the maximum attention value for consistent color scale\n",
    "    matrices = []\n",
    "    \n",
    "    # First pass to collect all matrices and find global max for consistent coloring\n",
    "    for layer in range(num_layers):\n",
    "        layer_matrices = []\n",
    "        for head in range(num_heads):\n",
    "            if isinstance(attention_weights[layer], torch.Tensor):\n",
    "                attn_shape = attention_weights[layer].shape\n",
    "                \n",
    "                if len(attn_shape) == 4:  # [batch_size, nhead, seq_len, seq_len]\n",
    "                    attn_matrix = attention_weights[layer][sample_idx, head].cpu().numpy()\n",
    "                elif len(attn_shape) == 3:  # [batch_size, seq_len, seq_len]\n",
    "                    attn_matrix = attention_weights[layer][sample_idx].cpu().numpy()\n",
    "                else:\n",
    "                    attn_matrix = np.ones((seq_length, seq_length)) / seq_length\n",
    "            else:\n",
    "                try:\n",
    "                    attn_matrix = attention_weights[layer][0][sample_idx, head].cpu().numpy()\n",
    "                except (TypeError, IndexError):\n",
    "                    attn_matrix = np.ones((seq_length, seq_length)) / seq_length\n",
    "            \n",
    "            layer_matrices.append(attn_matrix)\n",
    "            max_value = max(max_value, attn_matrix.max())\n",
    "        \n",
    "        matrices.append(layer_matrices)\n",
    "    \n",
    "    # Second pass to plot with consistent coloring\n",
    "    for layer in range(num_layers):\n",
    "        for head in range(num_heads):\n",
    "            ax = axes[layer, head]\n",
    "            attn_matrix = matrices[layer][head]\n",
    "            \n",
    "            # Create heatmap with consistent vmin/vmax for color scaling\n",
    "            sns.heatmap(attn_matrix, annot=False, cmap=cmap, ax=ax, \n",
    "                       cbar=False, vmin=0, vmax=max_value)\n",
    "            \n",
    "            # Configure the subplot\n",
    "            ax.set_title(f\"L{layer+1}H{head+1}\", fontsize=10)\n",
    "            \n",
    "            # Only show ticks for edge subplots\n",
    "            if layer == num_layers - 1:\n",
    "                ax.set_xticks([0, seq_length-1])\n",
    "                ax.set_xticklabels(['t0', f't{seq_length-1}'], fontsize=8)\n",
    "            else:\n",
    "                ax.set_xticks([])\n",
    "            \n",
    "            if head == 0:\n",
    "                ax.set_yticks([0, seq_length-1])\n",
    "                ax.set_yticklabels(['t0', f't{seq_length-1}'], fontsize=8)\n",
    "            else:\n",
    "                ax.set_yticks([])\n",
    "            \n",
    "            # Add row/column average magnitude indicators\n",
    "            # These small bars help identify which positions are attended to most\n",
    "            row_sum = attn_matrix.sum(axis=1)\n",
    "            col_sum = attn_matrix.sum(axis=0)\n",
    "            \n",
    "            # Highlight the diagonal for reference\n",
    "            for i in range(seq_length):\n",
    "                ax.add_patch(plt.Rectangle((i, i), 1, 1, fill=False, edgecolor='black', lw=0.5))\n",
    "            \n",
    "            # Mark the most attended position with a star\n",
    "            max_pos = np.unravel_index(np.argmax(attn_matrix), attn_matrix.shape)\n",
    "            ax.add_patch(plt.Circle((max_pos[1] + 0.5, max_pos[0] + 0.5), 0.4, \n",
    "                                   facecolor='none', edgecolor='red', linewidth=1))\n",
    "    \n",
    "    # Add a colorbar for the entire figure\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(0, max_value))\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "    cbar.set_label('Attention Weight')\n",
    "    \n",
    "    # Add a text explanation of attention patterns\n",
    "    fig.text(0.5, 0.02, \n",
    "            \"Each cell shows how much attention is given from a source token (y-axis) to a target token (x-axis).\\n\" +\n",
    "            \"Red circles mark the strongest attention connection in each head.\",\n",
    "            ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, right=0.9, wspace=0.1, hspace=0.2)\n",
    "    \n",
    "    # Save the figure\n",
    "    save_path = 'enhanced_all_attention_heads.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "def create_attention_pattern_summary(attention_weights, seq_length, num_layers, num_heads, \n",
    "                                    sample_idx=0, predicted_class=None, true_class=None):\n",
    "    \"\"\"\n",
    "    Creates a summary visualization of attention patterns focused on how they evolve\n",
    "    through layers and what positions they emphasize.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: List of attention weight tensors\n",
    "        seq_length: Length of input sequence\n",
    "        num_layers: Number of transformer layers\n",
    "        num_heads: Number of attention heads per layer\n",
    "        sample_idx: Index of the sample in the batch\n",
    "        predicted_class: Model's prediction for this sample\n",
    "        true_class: Ground truth label for this sample\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved visualization\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    gs = fig.add_gridspec(nrows=3, ncols=2, height_ratios=[1, 2, 1])\n",
    "    \n",
    "    # Create the first panel - mean attention across layers\n",
    "    ax_layers = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    # Extract and aggregate attention data across layers\n",
    "    layer_avg_attns = []\n",
    "    position_focus = np.zeros((num_layers, seq_length))\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        layer_attn = None\n",
    "        \n",
    "        if isinstance(attention_weights[layer], torch.Tensor):\n",
    "            attn_shape = attention_weights[layer].shape\n",
    "            \n",
    "            if len(attn_shape) == 4:  # [batch_size, nhead, seq_len, seq_len]\n",
    "                # Average across all heads\n",
    "                layer_attn = attention_weights[layer][sample_idx].mean(dim=0).cpu().numpy()\n",
    "            elif len(attn_shape) == 3:  # [batch_size, seq_len, seq_len]\n",
    "                layer_attn = attention_weights[layer][sample_idx].cpu().numpy()\n",
    "        else:\n",
    "            try:\n",
    "                # Average across all heads\n",
    "                head_attns = [attention_weights[layer][0][sample_idx, h].cpu().numpy() \n",
    "                            for h in range(num_heads)]\n",
    "                layer_attn = np.mean(head_attns, axis=0)\n",
    "            except (TypeError, IndexError):\n",
    "                layer_attn = np.ones((seq_length, seq_length)) / seq_length\n",
    "        \n",
    "        layer_avg_attns.append(layer_attn)\n",
    "        \n",
    "        # Track which positions each layer focuses on (column sums)\n",
    "        position_focus[layer] = layer_attn.sum(axis=0)\n",
    "    \n",
    "    # Plot how attention evolves through layers\n",
    "    layer_labels = [f\"Layer {i+1}\" for i in range(num_layers)]\n",
    "    sns.heatmap(position_focus, cmap=\"YlGnBu\", \n",
    "               xticklabels=[f\"t{i}\" for i in range(seq_length)],\n",
    "               yticklabels=layer_labels, ax=ax_layers)\n",
    "    \n",
    "    ax_layers.set_title(\"Position Focus Across Layers\", fontsize=14)\n",
    "    ax_layers.set_xlabel(\"Sequence Position\", fontsize=12)\n",
    "    ax_layers.set_ylabel(\"Layer\", fontsize=12)\n",
    "    \n",
    "    # Second panel - Layer-wise attention maps\n",
    "    ax_maps = fig.add_subplot(gs[1, :])\n",
    "    \n",
    "    # Create a grid of small attention maps\n",
    "    inner_gs = gs[1, :].subgridspec(1, num_layers, wspace=0.1)\n",
    "    axes_maps = [fig.add_subplot(inner_gs[0, i]) for i in range(num_layers)]\n",
    "    \n",
    "    for i, (ax, layer_attn) in enumerate(zip(axes_maps, layer_avg_attns)):\n",
    "        sns.heatmap(layer_attn, cmap=\"YlGnBu\", ax=ax, cbar=False)\n",
    "        ax.set_title(f\"Layer {i+1}\", fontsize=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Third panel - Head-wise attention pattern\n",
    "    ax_heads = fig.add_subplot(gs[2, 0])\n",
    "    ax_prediction = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    # Create a summary of head patterns from the final layer\n",
    "    head_position_focus = np.zeros((num_heads, seq_length))\n",
    "    \n",
    "    layer_idx = num_layers - 1  # Use the final layer\n",
    "    for head in range(num_heads):\n",
    "        if isinstance(attention_weights[layer_idx], torch.Tensor):\n",
    "            attn_shape = attention_weights[layer_idx].shape\n",
    "            \n",
    "            if len(attn_shape) == 4:  # [batch_size, nhead, seq_len, seq_len]\n",
    "                head_attn = attention_weights[layer_idx][sample_idx, head].cpu().numpy()\n",
    "            elif len(attn_shape) == 3:  # [batch_size, seq_len, seq_len]\n",
    "                head_attn = attention_weights[layer_idx][sample_idx].cpu().numpy()\n",
    "            else:\n",
    "                head_attn = np.ones((seq_length, seq_length)) / seq_length\n",
    "        else:\n",
    "            try:\n",
    "                head_attn = attention_weights[layer_idx][0][sample_idx, head].cpu().numpy()\n",
    "            except (TypeError, IndexError):\n",
    "                head_attn = np.ones((seq_length, seq_length)) / seq_length\n",
    "                \n",
    "        # Track which positions each head focuses on\n",
    "        head_position_focus[head] = head_attn.sum(axis=0)\n",
    "    \n",
    "    # Plot head-wise position focus\n",
    "    head_labels = [f\"Head {i+1}\" for i in range(num_heads)]\n",
    "    sns.heatmap(head_position_focus, cmap=\"YlGnBu\", \n",
    "               xticklabels=[f\"t{i}\" for i in range(seq_length)],\n",
    "               yticklabels=head_labels, ax=ax_heads)\n",
    "    \n",
    "    ax_heads.set_title(f\"Position Focus Across Heads (Layer {num_layers})\", fontsize=12)\n",
    "    ax_heads.set_xlabel(\"Sequence Position\", fontsize=10)\n",
    "    ax_heads.set_ylabel(\"Attention Head\", fontsize=10)\n",
    "    \n",
    "    # Final panel - Add prediction information and interpretation\n",
    "    ax_prediction.axis('off')\n",
    "    \n",
    "    # Create a text summary of the attention patterns\n",
    "    # Find which positions were most attended to overall\n",
    "    final_layer_focus = position_focus[-1]\n",
    "    most_attended_pos = np.argmax(final_layer_focus)\n",
    "    least_attended_pos = np.argmin(final_layer_focus)\n",
    "    \n",
    "    # Create the summary text\n",
    "    if predicted_class and true_class:\n",
    "        prediction_result = \"correct\" if predicted_class == true_class else \"incorrect\"\n",
    "        prediction_text = (\n",
    "            f\"PREDICTION SUMMARY\\n\\n\"\n",
    "            f\"Model prediction: {predicted_class}\\n\"\n",
    "            f\"Actual class: {true_class}\\n\"\n",
    "            f\"Result: {prediction_result.upper()}\\n\\n\"\n",
    "            f\"ATTENTION ANALYSIS\\n\\n\"\n",
    "            f\"• Most attended position: t{most_attended_pos}\\n\"\n",
    "            f\"• Least attended position: t{least_attended_pos}\\n\"\n",
    "            f\"• {num_heads} attention heads in {num_layers} layers\\n\\n\"\n",
    "            f\"The model's attention is primarily focused on\\n\"\n",
    "            f\"the {'beginning' if most_attended_pos < seq_length/3 else 'middle' if most_attended_pos < 2*seq_length/3 else 'end'} \"\n",
    "            f\"of the sequence, suggesting this region\\n\"\n",
    "            f\"contains the most discriminative features for\\n\"\n",
    "            f\"classifying this mudra gesture.\"\n",
    "        )\n",
    "    else:\n",
    "        prediction_text = (\n",
    "            f\"ATTENTION ANALYSIS\\n\\n\"\n",
    "            f\"• Most attended position: t{most_attended_pos}\\n\"\n",
    "            f\"• Least attended position: t{least_attended_pos}\\n\"\n",
    "            f\"• {num_heads} attention heads in {num_layers} layers\\n\\n\"\n",
    "            f\"The model's attention is primarily focused on\\n\"\n",
    "            f\"the {'beginning' if most_attended_pos < seq_length/3 else 'middle' if most_attended_pos < 2*seq_length/3 else 'end'} \"\n",
    "            f\"of the sequence.\"\n",
    "        )\n",
    "    \n",
    "    ax_prediction.text(0.05, 0.95, prediction_text, fontsize=11,\n",
    "                     verticalalignment='top', family='monospace')\n",
    "    \n",
    "    # Add a title for the entire figure\n",
    "    if predicted_class and true_class:\n",
    "        match_status = \"Correct ✓\" if predicted_class == true_class else \"Incorrect ✗\"\n",
    "        fig.suptitle(f\"Comprehensive Attention Analysis for {predicted_class} ({match_status})\", \n",
    "                   fontsize=16, fontweight='bold')\n",
    "    else:\n",
    "        fig.suptitle(\"Comprehensive Attention Analysis\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, hspace=0.3)\n",
    "    \n",
    "    # Save the figure\n",
    "    save_path = 'attention_pattern_summary.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e2ce05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Label encoder saved with 14 classes: ['Abhaya' 'Bhumisparsa' 'Dharmachakra' 'Dhyana' 'Jnana' 'Karana' 'Ksepana'\n",
      " 'Namaskara' 'No Mudra/No Movement/Transition' 'Tarjani'\n",
      " 'Unknown/Transition' 'Uttarabodhi' 'Varada' 'Vitarka']\n",
      "Dimensão de entrada: 24\n",
      "Número de classes: 14\n",
      "Classes: ['Abhaya' 'Bhumisparsa' 'Dharmachakra' 'Dhyana' 'Jnana' 'Karana' 'Ksepana'\n",
      " 'Namaskara' 'No Mudra/No Movement/Transition' 'Tarjani'\n",
      " 'Unknown/Transition' 'Uttarabodhi' 'Varada' 'Vitarka']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drodm\\AppData\\Local\\Temp\\ipykernel_75148\\3923582269.py:77: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')  # Forward fill para lidar com NaN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parâmetros do modelo: 203278\n",
      "Iniciando treinamento...\n",
      "Epoch 1/50:\n",
      "Train Loss: 0.9298, Train Acc: 0.7139\n",
      "Val Loss: 0.6855, Val Acc: 0.8002\n",
      "Epoch 2/50:\n",
      "Train Loss: 0.6845, Train Acc: 0.7892\n",
      "Val Loss: 0.5667, Val Acc: 0.8259\n",
      "Epoch 3/50:\n",
      "Train Loss: 0.6029, Train Acc: 0.8147\n",
      "Val Loss: 0.5113, Val Acc: 0.8495\n",
      "Epoch 4/50:\n",
      "Train Loss: 0.5692, Train Acc: 0.8268\n",
      "Val Loss: 0.4713, Val Acc: 0.8617\n",
      "Epoch 5/50:\n",
      "Train Loss: 0.5441, Train Acc: 0.8282\n",
      "Val Loss: 0.5099, Val Acc: 0.8591\n",
      "Epoch 6/50:\n",
      "Train Loss: 0.5335, Train Acc: 0.8370\n",
      "Val Loss: 0.4608, Val Acc: 0.8499\n",
      "Epoch 7/50:\n",
      "Train Loss: 0.5113, Train Acc: 0.8411\n",
      "Val Loss: 0.4561, Val Acc: 0.8652\n",
      "Epoch 8/50:\n",
      "Train Loss: 0.4985, Train Acc: 0.8433\n",
      "Val Loss: 0.4132, Val Acc: 0.8717\n",
      "Epoch 9/50:\n",
      "Train Loss: 0.4811, Train Acc: 0.8445\n",
      "Val Loss: 0.4363, Val Acc: 0.8569\n",
      "Epoch 10/50:\n",
      "Train Loss: 0.4874, Train Acc: 0.8490\n",
      "Val Loss: 0.4726, Val Acc: 0.8674\n",
      "Epoch 11/50:\n",
      "Train Loss: 0.4800, Train Acc: 0.8458\n",
      "Val Loss: 0.4477, Val Acc: 0.8722\n",
      "Epoch 12/50:\n",
      "Train Loss: 0.4645, Train Acc: 0.8502\n",
      "Val Loss: 0.4115, Val Acc: 0.8813\n",
      "Epoch 13/50:\n",
      "Train Loss: 0.4565, Train Acc: 0.8552\n",
      "Val Loss: 0.3805, Val Acc: 0.8787\n",
      "Epoch 14/50:\n",
      "Train Loss: 0.4490, Train Acc: 0.8577\n",
      "Val Loss: 0.3761, Val Acc: 0.8848\n",
      "Epoch 15/50:\n",
      "Train Loss: 0.4474, Train Acc: 0.8547\n",
      "Val Loss: 0.3738, Val Acc: 0.8805\n",
      "Epoch 16/50:\n",
      "Train Loss: 0.4367, Train Acc: 0.8585\n",
      "Val Loss: 0.3973, Val Acc: 0.8735\n",
      "Epoch 17/50:\n",
      "Train Loss: 0.4303, Train Acc: 0.8597\n",
      "Val Loss: 0.3762, Val Acc: 0.8866\n",
      "Epoch 18/50:\n",
      "Train Loss: 0.4309, Train Acc: 0.8606\n",
      "Val Loss: 0.4113, Val Acc: 0.8800\n",
      "Epoch 19/50:\n",
      "Train Loss: 0.4323, Train Acc: 0.8615\n",
      "Val Loss: 0.3892, Val Acc: 0.8813\n",
      "Epoch 20/50:\n",
      "Train Loss: 0.4184, Train Acc: 0.8635\n",
      "Val Loss: 0.3775, Val Acc: 0.8861\n",
      "Epoch 21/50:\n",
      "Train Loss: 0.4129, Train Acc: 0.8625\n",
      "Val Loss: 0.3856, Val Acc: 0.8787\n",
      "Epoch 22/50:\n",
      "Train Loss: 0.4128, Train Acc: 0.8660\n",
      "Val Loss: 0.3635, Val Acc: 0.8848\n",
      "Epoch 23/50:\n",
      "Train Loss: 0.4043, Train Acc: 0.8677\n",
      "Val Loss: 0.3541, Val Acc: 0.8901\n",
      "Epoch 24/50:\n",
      "Train Loss: 0.4035, Train Acc: 0.8646\n",
      "Val Loss: 0.3674, Val Acc: 0.8844\n",
      "Epoch 25/50:\n",
      "Train Loss: 0.3910, Train Acc: 0.8718\n",
      "Val Loss: 0.3557, Val Acc: 0.8887\n",
      "Epoch 26/50:\n",
      "Train Loss: 0.4088, Train Acc: 0.8651\n",
      "Val Loss: 0.3475, Val Acc: 0.8918\n",
      "Epoch 27/50:\n",
      "Train Loss: 0.3996, Train Acc: 0.8676\n",
      "Val Loss: 0.3508, Val Acc: 0.8901\n",
      "Epoch 28/50:\n",
      "Train Loss: 0.4078, Train Acc: 0.8672\n",
      "Val Loss: 0.3743, Val Acc: 0.8870\n",
      "Epoch 29/50:\n",
      "Train Loss: 0.3956, Train Acc: 0.8688\n",
      "Val Loss: 0.3357, Val Acc: 0.8975\n",
      "Epoch 30/50:\n",
      "Train Loss: 0.3921, Train Acc: 0.8732\n",
      "Val Loss: 0.3349, Val Acc: 0.8949\n",
      "Epoch 31/50:\n",
      "Train Loss: 0.3918, Train Acc: 0.8733\n",
      "Val Loss: 0.3544, Val Acc: 0.8896\n",
      "Epoch 32/50:\n",
      "Train Loss: 0.3904, Train Acc: 0.8708\n",
      "Val Loss: 0.3721, Val Acc: 0.8892\n",
      "Epoch 33/50:\n",
      "Train Loss: 0.3877, Train Acc: 0.8717\n",
      "Val Loss: 0.3617, Val Acc: 0.8861\n",
      "Epoch 34/50:\n",
      "Train Loss: 0.3946, Train Acc: 0.8718\n",
      "Val Loss: 0.3356, Val Acc: 0.8870\n",
      "Epoch 35/50:\n",
      "Train Loss: 0.3797, Train Acc: 0.8767\n",
      "Val Loss: 0.3575, Val Acc: 0.8848\n",
      "Epoch 36/50:\n",
      "Train Loss: 0.3770, Train Acc: 0.8736\n",
      "Val Loss: 0.3306, Val Acc: 0.8975\n",
      "Epoch 37/50:\n",
      "Train Loss: 0.3845, Train Acc: 0.8694\n",
      "Val Loss: 0.3636, Val Acc: 0.8870\n",
      "Epoch 38/50:\n",
      "Train Loss: 0.3783, Train Acc: 0.8743\n",
      "Val Loss: 0.3595, Val Acc: 0.8857\n",
      "Epoch 39/50:\n",
      "Train Loss: 0.3728, Train Acc: 0.8766\n",
      "Val Loss: 0.3633, Val Acc: 0.8905\n",
      "Early stopping após 39 épocas!\n",
      "Treinamento concluído!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drodm\\AppData\\Local\\Temp\\ipykernel_75148\\3923582269.py:345: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_mudra_transformer.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia final de validação: 0.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drodm\\AppData\\Local\\Temp\\ipykernel_75148\\2106185482.py:100: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  color=plt.cm.get_cmap(cmap)(row_sums/row_sums.max()))\n",
      "C:\\Users\\drodm\\AppData\\Local\\Temp\\ipykernel_75148\\2106185482.py:113: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  color=plt.cm.get_cmap(cmap)(col_sums/col_sums.max()))\n",
      "C:\\Users\\drodm\\AppData\\Local\\Temp\\ipykernel_75148\\2106185482.py:280: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced attention visualization saved to: enhanced_attn_layer4_head1.png\n",
      "All heads visualization saved to: enhanced_all_attention_heads.png\n",
      "Attention pattern summary saved to: attention_pattern_summary.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Definição do dispositivo para treinamento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# 1. Preparo dos dados\n",
    "\n",
    "class MudraDataset(Dataset):\n",
    "    def __init__(self, features, labels=None, seq_length=10, stride=1):\n",
    "        \"\"\"\n",
    "        Dataset para dados de tracking de mãos.\n",
    "        \n",
    "        Args:\n",
    "            features: Numpy array com as features (posição, rotação, curvatura)\n",
    "            labels: Numpy array com os rótulos (tipos de mudra) se disponível\n",
    "            seq_length: Tamanho da sequência para cada amostra\n",
    "            stride: Passo para criar sequências\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.seq_length = seq_length\n",
    "        self.stride = stride\n",
    "        self.indices = self._create_indices()\n",
    "        \n",
    "    def _create_indices(self):\n",
    "        \"\"\"Cria índices para sequências válidas com o comprimento desejado\"\"\"\n",
    "        indices = []\n",
    "        for i in range(0, len(self.features) - self.seq_length + 1, self.stride):\n",
    "            indices.append(i)\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.indices[idx]\n",
    "        end_idx = start_idx + self.seq_length\n",
    "        \n",
    "        # Obter a sequência de features\n",
    "        seq_features = self.features[start_idx:end_idx]\n",
    "        \n",
    "        # Se temos rótulos, retornar o rótulo correspondente (geralmente o último da sequência)\n",
    "        if self.labels is not None:\n",
    "            seq_label = self.labels[end_idx - 1]  # Último rótulo da sequência\n",
    "            return torch.FloatTensor(seq_features), torch.LongTensor([seq_label])\n",
    "        \n",
    "        return torch.FloatTensor(seq_features)\n",
    "\n",
    "def load_and_preprocess_data(file_path, seq_length=30, stride=5, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Carrega e pré-processa os dados de tracking de mãos.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Caminho para o arquivo CSV\n",
    "        seq_length: Tamanho da sequência para cada amostra\n",
    "        stride: Passo para criar sequências\n",
    "        test_size: Proporção do conjunto de teste\n",
    "        random_state: Semente para reprodutibilidade\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, val_loader, input_dim, num_classes, label_encoder\n",
    "    \"\"\"\n",
    "    # Carregar os dados\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.fillna(method='ffill')  # Forward fill para lidar com NaN\n",
    "    # Separar features e rótulos\n",
    "    features = df.drop(['Time', 'Mudra'], axis=1).values  # Remover colunas de tempo e rótulo\n",
    "    labels = df['Mudra'].values\n",
    "    \n",
    "    # Normalizar as features\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Codificar os rótulos\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Save the label encoder for later use\n",
    "    joblib.dump(label_encoder, 'mudra_label_encoder.joblib')\n",
    "    print(f\"Label encoder saved with {len(label_encoder.classes_)} classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    # Dividir em treino e validação\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        features, encoded_labels, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Criar datasets\n",
    "    train_dataset = MudraDataset(X_train, y_train, seq_length=seq_length, stride=stride)\n",
    "    val_dataset = MudraDataset(X_val, y_val, seq_length=seq_length, stride=stride)\n",
    "    \n",
    "    # Criar dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Dimensão de entrada (número de features)\n",
    "    input_dim = features.shape[1]\n",
    "    \n",
    "    # Número de classes\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    return train_loader, val_loader, input_dim, num_classes, label_encoder\n",
    "\n",
    "# 2. Arquitetura do Transformer modificado para capturar matrizes de atenção\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class CustomTransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "    \"\"\"Custom TransformerEncoderLayer that returns attention weights\"\"\"\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # self attention\n",
    "        src2, attn_weights = self._sa_block(src, src_mask, src_key_padding_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # feedforward network\n",
    "        src2 = self._ff_block(src)\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src, attn_weights\n",
    "    \n",
    "    def _sa_block(self, x, attn_mask, key_padding_mask):\n",
    "        x2, attn_weights = self.self_attn(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=True\n",
    "        )\n",
    "        return x2, attn_weights\n",
    "\n",
    "class CustomMultiheadAttention(nn.MultiheadAttention):\n",
    "    \"\"\"Modified MultiheadAttention that returns attention weights\"\"\"\n",
    "    \n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        return super().forward(query, key, value, key_padding_mask=key_padding_mask,\n",
    "                               need_weights=need_weights, attn_mask=attn_mask)\n",
    "\n",
    "class CustomTransformerEncoder(nn.Module):\n",
    "    \"\"\"Custom TransformerEncoder that returns attention weights\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super(CustomTransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        output = src\n",
    "        attention_weights = []\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output, attn_weights = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "            attention_weights.append(attn_weights)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class MudraTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, num_classes, dropout=0.1):\n",
    "        super(MudraTransformer, self).__init__()\n",
    "        \n",
    "        # Embedding da entrada\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Codificação posicional\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Camadas do Transformer customizadas para retornar matrizes de atenção\n",
    "        encoder_layer = CustomTransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False  # Importante para manter a compatibilidade\n",
    "        )\n",
    "        self.transformer_encoder = CustomTransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Camada de classificação (usamos apenas o último elemento da sequência)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        \n",
    "    def forward(self, x, src_mask=None, return_attention=False):\n",
    "        # x: [batch_size, seq_len, input_dim]\n",
    "        \n",
    "        # Aplicar embedding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Aplicar codificação posicional\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Aplicar dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transformar formato para o esperado pelo Transformer: [seq_len, batch_size, d_model]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Aplicar Transformer e capturar pesos de atenção\n",
    "        output, attention_weights = self.transformer_encoder(x, src_mask)\n",
    "        \n",
    "        # Transformar de volta: [batch_size, seq_len, d_model]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        \n",
    "        # Usar apenas o último elemento da sequência para classificação\n",
    "        output_last = output[:, -1, :]\n",
    "        \n",
    "        # Camada de classificação\n",
    "        logits = self.classifier(output_last)\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, attention_weights\n",
    "        return logits\n",
    "\n",
    "# 3. Treinamento e avaliação\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.squeeze().to(device)\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if torch.isnan(features).any():\n",
    "            print(\"Warning: NaN values found in features, skipping batch\")\n",
    "            continue\n",
    "            \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Skip batch if loss is NaN\n",
    "        if torch.isnan(loss).any():\n",
    "            print(\"Warning: NaN loss detected, skipping batch\")\n",
    "            continue\n",
    "        \n",
    "        # Backward pass e otimização\n",
    "        loss.backward()\n",
    "        \n",
    "        # Add gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Estatísticas\n",
    "        running_loss += loss.item() * features.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if total == 0:\n",
    "        return float('nan'), float('nan')\n",
    "        \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.squeeze().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Estatísticas\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience=10):\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Treinamento\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validação\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # Salvar o melhor modelo\n",
    "            torch.save(model.state_dict(), 'best_mudra_transformer.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping após {epoch+1} épocas!')\n",
    "                break\n",
    "    \n",
    "    # Carregar o melhor modelo\n",
    "    model.load_state_dict(torch.load('best_mudra_transformer.pth'))\n",
    "    return model\n",
    "\n",
    "# 4. Função para previsão e visualização de atenção\n",
    "\n",
    "def predict_with_attention(model, features, label_encoder, device):\n",
    "    \"\"\"\n",
    "    Faz previsões com o modelo treinado e retorna os pesos de atenção.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo treinado\n",
    "        features: Tensor de features [batch_size, seq_len, input_dim]\n",
    "        label_encoder: LabelEncoder usado para codificar as classes\n",
    "        device: Dispositivo para execução\n",
    "        \n",
    "    Returns:\n",
    "        Classe prevista, probabilidades e pesos de atenção\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs, attention_weights = model(features, return_attention=True)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Converter índices previstos para classes originais\n",
    "    predicted_classes = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
    "    \n",
    "    return predicted_classes, probabilities, attention_weights\n",
    "\n",
    "def plot_attention_matrix(attention_weights, seq_length, title=\"Matriz de Atenção\", layer_idx=0, head_idx=0, sample_idx=0):\n",
    "    \"\"\"\n",
    "    Plota a matriz de atenção para uma amostra específica.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: Lista de tensores de atenção de cada camada\n",
    "        seq_length: Comprimento da sequência\n",
    "        title: Título do gráfico\n",
    "        layer_idx: Índice da camada do transformer a ser visualizada\n",
    "        head_idx: Índice da cabeça de atenção a ser visualizada\n",
    "        sample_idx: Índice da amostra no batch a ser visualizada\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # FIX: The attention matrix format is different than expected\n",
    "    # Extract the attention matrix correctly based on how it's returned from the transformer\n",
    "    # Attention weights is a list of matrices with shape [batch_size, nhead, seq_len, seq_len]\n",
    "    # or [seq_len, seq_len] depending on the transformer implementation\n",
    "    \n",
    "    # Check the shape of attention_weights to handle appropriately\n",
    "    if isinstance(attention_weights[layer_idx], torch.Tensor):\n",
    "        # If it's a tensor, we need to extract the right dimensions\n",
    "        attn_shape = attention_weights[layer_idx].shape\n",
    "        \n",
    "        if len(attn_shape) == 4:  # [batch_size, nhead, seq_len, seq_len]\n",
    "            attn_matrix = attention_weights[layer_idx][sample_idx, head_idx].cpu().numpy()\n",
    "        elif len(attn_shape) == 3:  # [batch_size, seq_len, seq_len] (no separate heads)\n",
    "            attn_matrix = attention_weights[layer_idx][sample_idx].cpu().numpy()\n",
    "        else:  # Other formats\n",
    "            print(f\"Unexpected attention weight shape: {attn_shape}\")\n",
    "            # Try to adapt to the shape we have\n",
    "            if attn_shape[0] == seq_length and attn_shape[1] == seq_length:\n",
    "                # It's already a matrix of the right shape\n",
    "                attn_matrix = attention_weights[layer_idx].cpu().numpy()\n",
    "            else:\n",
    "                # Create a fallback matrix\n",
    "                attn_matrix = np.ones((seq_length, seq_length)) / seq_length\n",
    "                print(\"Using fallback attention matrix due to unexpected shape\")\n",
    "    else:\n",
    "        # If it's not a tensor (e.g., a tuple or other structure)\n",
    "        # Extract the attention part, assuming first element contains the attention weights\n",
    "        # This is common in some transformer implementations\n",
    "        try:\n",
    "            # Try to get the first element if it's a tuple or list\n",
    "            attn_matrix = attention_weights[layer_idx][0][sample_idx, head_idx].cpu().numpy()\n",
    "        except (TypeError, IndexError):\n",
    "            print(f\"Unable to extract attention matrix from type {type(attention_weights[layer_idx])}\")\n",
    "            # Create a fallback matrix\n",
    "            attn_matrix = np.ones((seq_length, seq_length)) / seq_length\n",
    "            print(\"Using fallback attention matrix due to extraction error\")\n",
    "    \n",
    "    # Criar um mapa de calor\n",
    "    sns.heatmap(attn_matrix, annot=False, cmap='viridis')\n",
    "    \n",
    "    # Configurar o gráfico\n",
    "    plt.title(f\"{title} - Camada {layer_idx+1}, Cabeça {head_idx+1}\")\n",
    "    plt.xlabel(\"Posição de Sequência (Destino)\")\n",
    "    plt.ylabel(\"Posição de Sequência (Origem)\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salvar o gráfico\n",
    "    plt.savefig(f'attention_matrix_layer{layer_idx+1}_head{head_idx+1}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return f'attention_matrix_layer{layer_idx+1}_head{head_idx+1}.png'\n",
    "\n",
    "def visualize_all_attention_heads(attention_weights, seq_length, num_layers, num_heads, sample_idx=0):\n",
    "    \"\"\"\n",
    "    Plota matrizes de atenção para todas as cabeças em todas as camadas.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: Lista de tensores de atenção de cada camada\n",
    "        seq_length: Comprimento da sequência\n",
    "        num_layers: Número de camadas do transformer\n",
    "        num_heads: Número de cabeças de atenção\n",
    "        sample_idx: Índice da amostra no batch a ser visualizada\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_layers, num_heads, figsize=(num_heads*3, num_layers*3))\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        for head in range(num_heads):\n",
    "            if num_layers > 1:\n",
    "                ax = axes[layer, head] if num_heads > 1 else axes[layer]\n",
    "            else:\n",
    "                ax = axes[head] if num_heads > 1 else axes\n",
    "            \n",
    "            # FIX: Extract attention matrix correctly as in the plot_attention_matrix function\n",
    "            if isinstance(attention_weights[layer], torch.Tensor):\n",
    "                attn_shape = attention_weights[layer].shape\n",
    "                \n",
    "                if len(attn_shape) == 4:  # [batch_size, nhead, seq_len, seq_len]\n",
    "                    attn_matrix = attention_weights[layer][sample_idx, head].cpu().numpy()\n",
    "                elif len(attn_shape) == 3:  # [batch_size, seq_len, seq_len] (no separate heads)\n",
    "                    # If no separate heads but trying to show multiple heads\n",
    "                    # Just show the same matrix for all heads\n",
    "                    attn_matrix = attention_weights[layer][sample_idx].cpu().numpy()\n",
    "                else:\n",
    "                    # Create a fallback matrix\n",
    "                    attn_matrix = np.ones((seq_length, seq_length)) / seq_length\n",
    "            else:\n",
    "                try:\n",
    "                    # Try to get the first element if it's a tuple or list\n",
    "                    attn_matrix = attention_weights[layer][0][sample_idx, head].cpu().numpy()\n",
    "                except (TypeError, IndexError):\n",
    "                    # Create a fallback matrix\n",
    "                    attn_matrix = np.ones((seq_length, seq_length)) / seq_length\n",
    "            \n",
    "            # Criar mapa de calor\n",
    "            sns.heatmap(attn_matrix, annot=False, cmap='viridis', ax=ax, cbar=False)\n",
    "            \n",
    "            # Configurar o subplot\n",
    "            ax.set_title(f\"L{layer+1}H{head+1}\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_attention_heads.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return 'all_attention_heads.png'\n",
    "\n",
    "# 5. Função principal\n",
    "\n",
    "def main(file_path='combined_one_hand_data_with_classification.csv'):\n",
    "    # Hiperparâmetros\n",
    "    seq_length = 10        # Tamanho da sequência\n",
    "    stride = 5             # Passo para criação de sequências\n",
    "    d_model = 128          # Dimensão do modelo\n",
    "    nhead = 8              # Número de cabeças de atenção\n",
    "    num_encoder_layers = 4 # Número de camadas do encoder\n",
    "    dim_feedforward = 512  # Dimensão da camada feed-forward\n",
    "    dropout = 0.2          # Taxa de dropout\n",
    "    learning_rate = 0.001  # Taxa de aprendizado\n",
    "    num_epochs = 50        # Número de épocas\n",
    "    patience = 10          # Paciência para early stopping\n",
    "    \n",
    "    # Carregar e pré-processar os dados\n",
    "    train_loader, val_loader, input_dim, num_classes, label_encoder = load_and_preprocess_data(\n",
    "        file_path, seq_length=seq_length, stride=stride\n",
    "    )\n",
    "    \n",
    "    print(f\"Dimensão de entrada: {input_dim}\")\n",
    "    print(f\"Número de classes: {num_classes}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    # Inicializar o modelo\n",
    "    model = MudraTransformer(\n",
    "        input_dim=input_dim,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        num_classes=num_classes,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Mostrar resumo do modelo (número de parâmetros)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total de parâmetros do modelo: {total_params}\")\n",
    "    \n",
    "    # Definir função de perda e otimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    print(\"Iniciando treinamento...\")\n",
    "    model = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        num_epochs, device, patience=patience\n",
    "    )\n",
    "    print(\"Treinamento concluído!\")\n",
    "    \n",
    "    # Avaliar no conjunto de validação\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    print(f\"Acurácia final de validação: {val_acc:.4f}\")\n",
    "    \n",
    "    # Exemplo de como fazer previsões e visualizar matrizes de atenção\n",
    "    for features, labels in val_loader:\n",
    "        batch_size = features.size(0)\n",
    "        predicted_classes, probabilities, attention_weights = predict_with_attention(\n",
    "            model, features, label_encoder, device\n",
    "        )\n",
    "        \n",
    "        # Get sample information\n",
    "        sample_idx = 0  # First sample in batch\n",
    "        true_class = label_encoder.inverse_transform([labels[sample_idx].item()])[0]\n",
    "        predicted_class = predicted_classes[sample_idx]\n",
    "        \n",
    "        # Visualize with enhanced functions\n",
    "        enhanced_attn_file = enhanced_attention_matrix(\n",
    "            attention_weights, \n",
    "            seq_length,\n",
    "            title=\"Mudra Attention Pattern\",\n",
    "            layer_idx=num_encoder_layers-1,  # Last layer\n",
    "            head_idx=0,  # First head\n",
    "            sample_idx=sample_idx,\n",
    "            predicted_class=predicted_class,\n",
    "            true_class=true_class\n",
    "        )\n",
    "        \n",
    "        # Create all-heads visualization\n",
    "        all_heads_file = visualize_all_attention_heads_enhanced(\n",
    "            attention_weights,\n",
    "            seq_length,\n",
    "            num_encoder_layers,\n",
    "            nhead,\n",
    "            sample_idx=sample_idx,\n",
    "            predicted_class=predicted_class,\n",
    "            true_class=true_class\n",
    "        )\n",
    "        \n",
    "        # Create a comprehensive summary\n",
    "        summary_file = create_attention_pattern_summary(\n",
    "            attention_weights,\n",
    "            seq_length,\n",
    "            num_encoder_layers,\n",
    "            nhead,\n",
    "            sample_idx=sample_idx,\n",
    "            predicted_class=predicted_class,\n",
    "            true_class=true_class\n",
    "        )\n",
    "        \n",
    "        print(f\"Enhanced attention visualization saved to: {enhanced_attn_file}\")\n",
    "        print(f\"All heads visualization saved to: {all_heads_file}\")\n",
    "        print(f\"Attention pattern summary saved to: {summary_file}\")\n",
    "        \n",
    "        break  # Just process one batch for example\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Substitua pelo caminho real do seu arquivo CSV\n",
    "    main(file_path='combined_one_hand_data_with_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7deb5db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sequence from index 3000 to 3009\n",
      "Actual label: Dharmachakra\n",
      "Predicted label: Dharmachakra\n",
      "\n",
      "Class probabilities:\n",
      "Abhaya: 0.0000\n",
      "Bhumisparsa: 0.0004\n",
      "Dharmachakra: 0.9554\n",
      "Dhyana: 0.0001\n",
      "Jnana: 0.0042\n",
      "Karana: 0.0001\n",
      "Ksepana: 0.0002\n",
      "Namaskara: 0.0001\n",
      "No Mudra/No Movement/Transition: 0.0378\n",
      "Tarjani: 0.0000\n",
      "Unknown/Transition: 0.0004\n",
      "Uttarabodhi: 0.0001\n",
      "Varada: 0.0001\n",
      "Vitarka: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drodm\\AppData\\Local\\Temp\\ipykernel_75148\\3569704425.py:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')  # Forward fill para lidar com NaN\n",
      "C:\\Users\\drodm\\AppData\\Local\\Temp\\ipykernel_75148\\3569704425.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv('combined_one_hand_data_with_classification.csv')\n",
    "df = df.fillna(method='ffill')  # Forward fill para lidar com NaN   \n",
    "\n",
    "\n",
    "# Load the trained model and label encoder\n",
    "model_path = 'best_mudra_transformer.pth'\n",
    "label_encoder_path = 'mudra_label_encoder.joblib'\n",
    "\n",
    "# Load the label encoder\n",
    "label_encoder = joblib.load(label_encoder_path)\n",
    "\n",
    "# Set parameters to match the model training\n",
    "seq_length = 10\n",
    "input_dim = len(df.columns) - 2  # All columns except Time and Mudra\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_encoder_layers = 4\n",
    "dim_feedforward = 512\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Initialize the model with the same architecture\n",
    "model = MudraTransformer(\n",
    "    input_dim=input_dim,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Sample a sequence from the dataframe\n",
    "sample_idx = 3000 # Choose an arbitrary index\n",
    "sample_features = df.drop(['Time', 'Mudra'], axis=1).values[sample_idx:sample_idx+seq_length]\n",
    "sample_label = df['Mudra'].values[sample_idx+seq_length-1]\n",
    "\n",
    "# Normalize the features (similar to training)\n",
    "scaler = StandardScaler()\n",
    "# Using multiple samples to get a better fit for the scaler\n",
    "scaler.fit(df.drop(['Time', 'Mudra'], axis=1).values)\n",
    "normalized_features = scaler.transform(sample_features)\n",
    "\n",
    "# Convert to tensor and add batch dimension\n",
    "input_tensor = torch.FloatTensor(normalized_features).unsqueeze(0).to(device)  # [1, seq_length, input_dim]\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class = label_encoder.inverse_transform(predicted.cpu().numpy())[0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Sample sequence from index {sample_idx} to {sample_idx+seq_length-1}\")\n",
    "print(f\"Actual label: {sample_label}\")\n",
    "print(f\"Predicted label: {predicted_class}\")\n",
    "print(\"\\nClass probabilities:\")\n",
    "for i, prob in enumerate(probabilities[0].cpu().numpy()):\n",
    "    class_name = label_encoder.inverse_transform([i])[0]\n",
    "    print(f\"{class_name}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
